{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "VoiceSplit-Demo-Si-SNR-with-upit-GE2E-GE2E-CorentinJ-trained-with-3k hours.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpe7OW6UAQ6X"
      },
      "source": [
        "This is a noteboook used to generate the speaker embeddings with the  GE2E model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn3Z5JQUA6FH"
      },
      "source": [
        "! git clone https://github.com/Edresson/VoiceSplit.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGGVwCx6BKql"
      },
      "source": [
        "# Note: Before install you need restart colab section\n",
        "! pip install -U tqdm numpy librosa mir_eval matplotlib Pillow  tensorboardX pandas torchaudio PyYAML pysoundfile ffmpeg-normalize\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFs3Y6AYd_BE"
      },
      "source": [
        "! apt install ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFv-LKdUBhVy"
      },
      "source": [
        "# Check gpu \n",
        "import torch\n",
        "torch.cuda.current_device()\n",
        "torch.cuda.device_count()\n",
        "torch.cuda.is_available()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MlJqyF6DXkm"
      },
      "source": [
        "# clone GE2E Embedder\n",
        "! git clone https://github.com/Edresson/GE2E-Speaker-Encoder.git\n",
        "#Install Requeriments\n",
        "!python -m pip install umap-learn visdom webrtcvad librosa>=0.5.1 matplotlib>=2.0.2 numpy>=1.14.0  scipy>=1.0.0  tqdm sounddevice Unidecode inflect multiprocess numba\n",
        "\n",
        "\n",
        "#Download encoder Checkpoint\n",
        "!wget https://github.com/Edresson/Real-Time-Voice-Cloning/releases/download/checkpoints/pretrained.zip\n",
        "!unzip pretrained.zip\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "99GRt8CxAQ6a"
      },
      "source": [
        "import sys \n",
        "sys.path.insert(0, \"./VoiceSplit/\")\n",
        "sys.path.insert(0, \"./GE2E-Speaker-Encoder/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xroaJNc1AQ6m"
      },
      "source": [
        "# Imports from GE2E\n",
        "from encoder import inference as encoder\n",
        "from encoder.params_model import model_embedding_size as speaker_embedding_size\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# Imports from VoiceSplit model\n",
        "from utils.audio_processor import WrapperAudioProcessor as AudioProcessor \n",
        "from utils.generic_utils import load_config\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "from models.voicefilter.model import VoiceFilter\n",
        "from models.voicesplit.model import VoiceSplit\n",
        "\n",
        "from utils.generic_utils import load_config, load_config_from_str\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "20_CEgOOAQ6z"
      },
      "source": [
        "#Load and test GE2E model\n",
        "print(\"Preparing the encoder, the synthesizer and the vocoder...\")\n",
        "encoder.load_model(Path('encoder/saved_models/pretrained.pt'))\n",
        "print(\"Testing your configuration with small inputs.\")\n",
        "# Forward an audio waveform of zeroes that lasts 1 second. Notice how we can get the encoder's\n",
        "# sampling rate, which may differ.\n",
        "# If you're unfamiliar with digital audio, know that it is encoded as an array of floats \n",
        "# (or sometimes integers, but mostly floats in this projects) ranging from -1 to 1.\n",
        "# The sampling rate is the number of values (samples) recorded per second, it is set to\n",
        "# 16000 for the encoder. Creating an array of length <sampling_rate> will always correspond \n",
        "# to an audio of 1 second.\n",
        "print(\"\\tTesting the encoder...\")\n",
        "\n",
        "wav = np.zeros(encoder.sampling_rate)    \n",
        "embed = encoder.embed_utterance(wav)\n",
        "print(embed.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg1BOMRgAQ7n"
      },
      "source": [
        "def get_embedding(encoder, ap, wave_file_path):\n",
        "  preprocessed_wav = encoder.preprocess_wav(wave_file_path)\n",
        "  file_embedding = encoder.embed_utterance(preprocessed_wav)\n",
        "  return torch.from_numpy(file_embedding.reshape(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ckxs1ziAQ7u"
      },
      "source": [
        "# Download VoiceSplit checkpoint\n",
        "!wget https://github.com/Edresson/VoiceSplit/releases/download/checkpoints/voiceSplit-trained-with-Si-SRN-GE2E-CorintinJ-best_checkpoint.pt -O best_checkpoint.pt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8wn9YsXJAAG"
      },
      "source": [
        "# Paths\n",
        "checkpoint_path = 'best_checkpoint.pt'\n",
        "# load checkpoint \n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "#load config from checkpoint\n",
        "c = load_config_from_str(checkpoint['config_str'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPTSZ7ymLm0l"
      },
      "source": [
        "ap = AudioProcessor(c.audio) # create AudioProcessor for model\n",
        "model_name = c.model_name\n",
        "cuda = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWhJP3nQLw6W"
      },
      "source": [
        "# load model\n",
        "if(model_name == 'voicefilter'):\n",
        "    print('inicializado com voicefilter')\n",
        "    model = VoiceFilter(c)\n",
        "elif(model_name == 'voicesplit'):\n",
        "    model = VoiceSplit(c)\n",
        "else:\n",
        "    raise Exception(\" The model '\"+model_name+\"' is not suported\")\n",
        "\n",
        "if c.train_config['optimizer'] == 'adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                  lr=c.train_config['learning_rate'])\n",
        "else:\n",
        "    raise Exception(\"The %s  not is a optimizer supported\" % c.train['optimizer'])\n",
        "\n",
        "      \n",
        "model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "step = checkpoint['step']\n",
        "\n",
        "print(\"load model form Step:\", step)\n",
        "# convert model from cuda\n",
        "if cuda:\n",
        "    model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-iK9FA8w6Hg"
      },
      "source": [
        "# utils for plot spectrogram\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "def fig2np(fig):\n",
        "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    return data\n",
        "\n",
        "def plot_spectrogram_to_numpy(spectrogram):\n",
        "    fig, ax = plt.subplots(figsize=(12, 3))\n",
        "    im = ax.imshow(spectrogram, aspect='auto', origin='lower',\n",
        "                   interpolation='none')\n",
        "    plt.colorbar(im, ax=ax)\n",
        "    plt.xlabel('Frames')\n",
        "    plt.ylabel('Channels')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig.canvas.draw()\n",
        "    data = fig2np(fig)\n",
        "    plt.close()\n",
        "    return data\n",
        "\n",
        "def save_spec(path, spec):\n",
        "  data = plot_spectrogram_to_numpy(spec)\n",
        "  imageio.imwrite(path, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6DpYkT4eXqr"
      },
      "source": [
        "# utils for calculate SNR and SDR\n",
        "# this code is adpated from https://github.com/JusperLee/Calculate-SNR-SDR/\n",
        "import torch\n",
        "from mir_eval.separation import bss_eval_sources\n",
        "from itertools import permutations\n",
        "\n",
        "def SI_SNR(_s, s, mix, zero_mean=True):\n",
        "    '''\n",
        "         Calculate the SNR indicator between the two audios. \n",
        "         The larger the value, the better the separation.\n",
        "         input:\n",
        "               _s: Generated audio\n",
        "               s:  Ground Truth audio\n",
        "         output:\n",
        "               SNR value \n",
        "    '''\n",
        "    length = _s.shape[0]\n",
        "    _s = _s[:length]\n",
        "    s =s[:length]\n",
        "    mix = mix[:length]\n",
        "    if zero_mean:\n",
        "        _s = _s - torch.mean(_s)\n",
        "        s = s - torch.mean(s)\n",
        "        mix = mix - torch.mean(mix)\n",
        "    s_target = sum(torch.mul(_s, s))*s/(torch.pow(torch.norm(s, p=2), 2)+1e-8)\n",
        "    e_noise = _s - s_target\n",
        "    # mix ---------------------------\n",
        "    mix_target = sum(torch.mul(mix, s))*s/(torch.pow(torch.norm(s, p=2), 2)+1e-8)\n",
        "    mix_noise = mix - mix_target \n",
        "    return 20*torch.log10(torch.norm(s_target, p=2)/(torch.norm(e_noise, p=2)+1e-8)) - 20*torch.log10(torch.norm(mix_target, p=2)/(torch.norm(mix_noise, p=2)+1e-8))\n",
        "\n",
        "\n",
        "def permute_SI_SNR(_s_lists, s_lists, mix):\n",
        "    '''\n",
        "        Calculate all possible SNRs according to \n",
        "        the permutation combination and \n",
        "        then find the maximum value.\n",
        "        input:\n",
        "               _s_lists: Generated audio list\n",
        "               s_lists: Ground truth audio list\n",
        "        output:\n",
        "               max of SI-SNR\n",
        "    '''\n",
        "    length = len(_s_lists)\n",
        "    results = []\n",
        "    per = []\n",
        "    for p in permutations(range(length)):\n",
        "        s_list = [s_lists[n] for n in p]\n",
        "        result = sum([SI_SNR(_s, s, mix, zero_mean=True) for _s, s in zip(_s_lists, s_list)])/length\n",
        "        results.append(result)\n",
        "        per.append(p)\n",
        "    return max(results), per[results.index(max(results))]\n",
        "\n",
        "\n",
        "def SDR(est, egs, mix):\n",
        "    '''\n",
        "        calculate SDR\n",
        "        est: Network generated audio\n",
        "        egs: Ground Truth\n",
        "    '''\n",
        "    length = est.numpy().shape[0]\n",
        "    sdr, _, _, _ = bss_eval_sources(egs.numpy()[:length], est.numpy()[:length])\n",
        "    mix_sdr, _, _, _ = bss_eval_sources(egs.numpy()[:length], mix.numpy()[:length])\n",
        "    return float(sdr-mix_sdr)\n",
        "\n",
        "\n",
        "def permutation_sdr(est_list, egs_list, mix, per):\n",
        "    n = len(est_list)\n",
        "    result = sum([SDR(est_list[a], egs_list[b], mix)\n",
        "                      for a, b in enumerate(per)])/n\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol6NJFK9eYuq"
      },
      "source": [
        "# extract caracteristics\n",
        "def normalise_and_extract_features(encoder, ap, mixed_path, target_path, target_path2, emb_ref_path):\n",
        "  mixed_path_norm = mixed_path.replace('.wav','-norm.wav') \n",
        "  target_path_norm = target_path.replace('.wav','-norm.wav')\n",
        "  target_path_norm2 = target_path2.replace('.wav','-norm.wav')\n",
        "  emb_ref_path_norm = emb_ref_path.replace('.wav','-norm.wav')\n",
        "  \n",
        "  # normalise wavs\n",
        "  ! ffmpeg-normalize $mixed_path -ar 16000 -o $mixed_path_norm -f\n",
        "  ! ffmpeg-normalize  $target_path -ar 16000 -o $target_path_norm -f \n",
        "  ! ffmpeg-normalize  $target_path2 -ar 16000 -o $target_path_norm2 -f \n",
        "  ! ffmpeg-normalize  $emb_ref_path -ar 16000 -o $emb_ref_path_norm -f\n",
        "\n",
        "  # load wavs\n",
        "  target_wav = ap.load_wav(target_path_norm)\n",
        "  target_wav2 = ap.load_wav(target_path_norm2)\n",
        "  mixed_wav = ap.load_wav(mixed_path_norm)\n",
        "  emb_wav = ap.load_wav(emb_ref_path_norm)\n",
        "  \n",
        "  # trim initial and end  wave file silence using librosa\n",
        "  # target_wav, _ = librosa.effects.trim(target_wav, top_db=20)\n",
        "  # mixed_wav, _ = librosa.effects.trim(mixed_wav, top_db=20)\n",
        "  # emb_wav, _ = librosa.effects.trim(emb_wav, top_db=20)\n",
        "\n",
        "  # normalise wavs\n",
        "  norm_factor = np.max(np.abs(mixed_wav)) * 1.1\n",
        "  mixed_wav = mixed_wav/norm_factor\n",
        "  emb_wav = emb_wav/norm_factor\n",
        "  target_wav = target_wav/norm_factor\n",
        "  target_wav2 = target_wav2/norm_factor\n",
        "\n",
        "  # save embedding ref \n",
        "  librosa.output.write_wav(emb_ref_path_norm, emb_wav, 16000)\n",
        "  # save this is necessary for demo\n",
        "  librosa.output.write_wav(mixed_path_norm, mixed_wav, 16000)\n",
        "  librosa.output.write_wav(target_path_norm, target_wav, 16000)\n",
        "  librosa.output.write_wav(target_path_norm2, target_wav2, 16000)\n",
        "\n",
        "  embedding = get_embedding(encoder, ap, emb_ref_path_norm)\n",
        "  mixed_spec, mixed_phase = ap.get_spec_from_audio(mixed_wav, return_phase=True)\n",
        "  return embedding, mixed_spec, mixed_phase, target_wav, target_wav2, mixed_wav, emb_wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrJnmo2leelK"
      },
      "source": [
        "def predict(encoder, ap, mixed_path, target_path, target_path2, emb_ref_path, outpath='predict.wav', save_img=False):\n",
        "  embedding, mixed_spec, mixed_phase, target_wav, target_wav2, mixed_wav, emb_wav = normalise_and_extract_features(encoder, ap, mixed_path, target_path, target_path2,  emb_ref_path)\n",
        "  # use the model\n",
        "  mixed_spec = torch.from_numpy(mixed_spec).float()\n",
        "\n",
        "  # append 1 dimension on mixed, its need because the model spected batch\n",
        "  mixed_spec = mixed_spec.unsqueeze(0)\n",
        "  embedding = embedding.unsqueeze(0)\n",
        "\n",
        "  if cuda:\n",
        "    embedding = embedding.cuda()\n",
        "    mixed_spec = mixed_spec.cuda()\n",
        "\n",
        "  mask = model(mixed_spec, embedding)\n",
        "  output = mixed_spec * mask\n",
        "\n",
        "  # inverse spectogram to wav\n",
        "  est_mag = output[0].cpu().detach().numpy()\n",
        "  mixed_spec = mixed_spec[0].cpu().detach().numpy()\n",
        "  # use phase from mixed wav for reconstruct the wave\n",
        "  est_wav = ap.inv_spectrogram(est_mag, phase=mixed_phase)\n",
        "\n",
        "  librosa.output.write_wav(outpath, est_wav, 16000)\n",
        "  if save_img:\n",
        "      img_path = outpath.replace('predict', 'images').replace(' ', '').replace('.wav','-est.png')\n",
        "      save_spec(img_path, est_mag)\n",
        "      target_mag = ap.get_spec_from_audio(target_wav, return_phase=False)\n",
        "      img_path = outpath.replace('predict', 'images').replace(' ', '').replace('.wav','-target.png')\n",
        "      save_spec(img_path, target_mag)\n",
        "      img_path = outpath.replace('predict', 'images').replace(' ', '').replace('.wav','-mixed.png')\n",
        "      save_spec(img_path, mixed_spec)\n",
        "      \n",
        "\n",
        "  return est_wav, target_wav, target_wav2, mixed_wav, emb_wav\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqnI9S9Bep_u"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import Audio, display\n",
        "from mir_eval.separation import bss_eval_sources\n",
        "import numpy as np\n",
        "# create output path\n",
        "os.makedirs('VoiceSplit/datasets/LibriSpeech/audios_demo/2_speakers/predict/',exist_ok=True)\n",
        "os.makedirs('VoiceSplit/datasets/LibriSpeech/audios_demo/2_speakers/images/',exist_ok=True)\n",
        "\n",
        "test_csv = pd.read_csv('VoiceSplit/datasets/LibriSpeech/test_demo.csv', sep=',').values\n",
        "\n",
        "sdrs_before = []\n",
        "sdrs_after = []\n",
        "snrs_before = []\n",
        "snrs_after = []\n",
        "for noise_utterance,emb_utterance, clean_utterance, clean_utterance2 in test_csv:\n",
        "  noise_utterance = os.path.join('VoiceSplit',noise_utterance).replace(' ', '')\n",
        "  emb_utterance = os.path.join('VoiceSplit',emb_utterance).replace(' ', '')\n",
        "  clean_utterance = os.path.join('VoiceSplit',clean_utterance).replace(' ', '')\n",
        "  clean_utterance2 = os.path.join('VoiceSplit',clean_utterance2).replace(' ', '')\n",
        "  output_path = noise_utterance.replace('noisy', 'predict').replace(' ', '')\n",
        "  est_wav, target_wav, target_wav2, mixed_wav, emb_wav = predict(encoder, ap, noise_utterance, clean_utterance, clean_utterance2, emb_utterance, outpath=output_path, save_img=True)\n",
        "\n",
        "  len_est = len(est_wav)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest \n",
        "    est_wav = est_wav[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    est_wav = np.pad(est_wav, (0, len(mixed_wav)-len(est_wav)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  # get wav for second voice, its need for SDR calculation\n",
        "  est_wav2 = mixed_wav-est_wav\n",
        "\n",
        "  len_est = len(est_wav2)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest\n",
        "    est_wav2 = est_wav2[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    est_wav2 = np.pad(est_wav2, (0, len(mixed_wav)-len(est_wav2)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  len_est = len(target_wav)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest\n",
        "    target_wav = target_wav[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    target_wav = np.pad(target_wav, (0, len(mixed_wav)-len(target_wav)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  # get target_wav for second voice, its recomended because google dont provide clean_utterance2 in your demo i need get in LibreSpeech Dataset, but i dont know if they normalised this file..\n",
        "  target_wav2 = mixed_wav - target_wav\n",
        "  '''len_est = len(target_wav2)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest\n",
        "    target_wav2 = target_wav2[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    target_wav2 = np.pad(target_wav2, (0, len(mixed_wav)-len(target_wav2)), 'constant', constant_values=(0, 0))'''\n",
        "\n",
        "  # calculate snr and sdr before model\n",
        "  ests = [torch.from_numpy(mixed_wav), torch.from_numpy(mixed_wav)] # the same voices is mixed_wav\n",
        "  egs = [torch.from_numpy(target_wav), torch.from_numpy(target_wav2)]\n",
        "  mix = torch.from_numpy(mixed_wav)\n",
        "  _snr, per = permute_SI_SNR(ests, egs, mix)\n",
        "  _sdr = permutation_sdr(ests, egs, mix, per)\n",
        "  snrs_before.append(_snr)\n",
        "  sdrs_before.append(_sdr)\n",
        "\n",
        "  # calculate snr and sdr after model\n",
        "  ests = [torch.from_numpy(est_wav), torch.from_numpy(est_wav2)]\n",
        "  egs = [torch.from_numpy(target_wav), torch.from_numpy(target_wav2)]\n",
        "  mix = torch.from_numpy(mixed_wav)\n",
        "  _snr, per = permute_SI_SNR(ests, egs, mix)\n",
        "  _sdr = permutation_sdr(ests, egs, mix, per)\n",
        "  snrs_after.append(_snr)\n",
        "  sdrs_after.append(_sdr)\n",
        "\n",
        "  # show in notebook results\n",
        "  print('-'*100)\n",
        "  print('-'*30,os.path.basename(noise_utterance),'-'*30)\n",
        "  print(\"Input/Noise Audio\")\n",
        "  display(Audio(mixed_wav,rate=16000))\n",
        "  print('Predicted Audio')\n",
        "  display(Audio(est_wav,rate=16000))\n",
        "  print('Target Audio')\n",
        "  display(Audio(target_wav,rate=16000))\n",
        "  print('Predicted2 Audio')\n",
        "  display(Audio(est_wav2,rate=16000))\n",
        "  print('Target2 Audio')\n",
        "  display(Audio(target_wav2,rate=16000))\n",
        "  print('-'*100)\n",
        "  del target_wav, est_wav, mixed_wav\n",
        "\n",
        "\n",
        "print('='*20,\"Before Model\",'='*20)\n",
        "print('\\nAverage SNRi: {:.5f}'.format(np.array(snrs_before).mean()))\n",
        "print('Average SDRi: {:.5f}'.format(np.array(sdrs_before).mean()))\n",
        "\n",
        "print('='*20,\"After Model\",'='*20)\n",
        "print('\\nAverage SNRi: {:.5f}'.format(np.array(snrs_after).mean()))\n",
        "print('Average SDRi: {:.5f}'.format(np.array(sdrs_after).mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEJnJxKu7i0h"
      },
      "source": [
        "! zip -r audios_demo_single_best_model.zip VoiceSplit/datasets/LibriSpeech/audios_demo/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oYSmYYIjuHH"
      },
      "source": [
        "# Apply VoiceFilter on clean audio (single speaker)\n",
        "import pandas as pd\n",
        "from IPython.display import Audio, display\n",
        "from mir_eval.separation import bss_eval_sources\n",
        "import numpy as np\n",
        "# create output path\n",
        "os.makedirs('VoiceSplit/datasets/LibriSpeech/audios_demo/single_speaker/predict/',exist_ok=True)\n",
        "os.makedirs('VoiceSplit/datasets/LibriSpeech/audios_demo/single_speaker/images/',exist_ok=True)\n",
        "test_csv = pd.read_csv('VoiceSplit/datasets/LibriSpeech/test_demo.csv', sep=',').values\n",
        "\n",
        "sdrs_before = []\n",
        "sdrs_after = []\n",
        "snrs_before = []\n",
        "snrs_after = []\n",
        "for _ ,emb_utterance, clean_utterance, clean_utterance2 in test_csv:\n",
        "  emb_utterance = os.path.join('VoiceSplit',emb_utterance).replace(' ', '')\n",
        "  clean_utterance = os.path.join('VoiceSplit',clean_utterance).replace(' ', '')\n",
        "  clean_utterance2 = os.path.join('VoiceSplit',clean_utterance2).replace(' ', '')\n",
        "  output_path = clean_utterance.replace('/clean/', '/single_speaker/predict/').replace(' ', '')\n",
        "\n",
        "  #  input = clean uterrance\n",
        "  est_wav, target_wav, target_wav2, mixed_wav, emb_wav = predict(encoder, ap, clean_utterance, clean_utterance, clean_utterance2, emb_utterance, outpath=output_path, save_img=True)\n",
        "\n",
        "  len_est = len(est_wav)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest \n",
        "    est_wav = est_wav[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    est_wav = np.pad(est_wav, (0, len(mixed_wav)-len(est_wav)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  # get wav for second voice, its need for SDR calculation\n",
        "  est_wav2 = mixed_wav-est_wav\n",
        "\n",
        "  len_est = len(est_wav2)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest\n",
        "    est_wav2 = est_wav2[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    est_wav2 = np.pad(est_wav2, (0, len(mixed_wav)-len(est_wav2)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  len_est = len(target_wav)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest\n",
        "    target_wav = target_wav[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    target_wav = np.pad(target_wav, (0, len(mixed_wav)-len(target_wav)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  # show in notebook results\n",
        "  print('-'*100)\n",
        "  print('-'*30,os.path.basename(noise_utterance),'-'*30)\n",
        "  print(\"Input/Clean Audio\")\n",
        "  display(Audio(mixed_wav,rate=16000))\n",
        "  print('Predicted Audio')\n",
        "  display(Audio(est_wav,rate=16000))\n",
        "  print('-'*100)\n",
        "  del target_wav, est_wav, mixed_wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW743nTse2eo"
      },
      "source": [
        "# in google paper dont is reported SNRi, and not is clean for my when we calculate SNR, for this reason i calculate this\n",
        "# NOTE: its use other speaker encoder, and other normalization on wavs, for this reason its not directly comparable.\n",
        "import pandas as pd\n",
        "from IPython.display import Audio, display\n",
        "from mir_eval.separation import bss_eval_sources\n",
        "import numpy as np\n",
        "# SDR from google paper for this instances\n",
        "test_csv = pd.read_csv('VoiceSplit/datasets/LibriSpeech/test_demo.csv', sep=',').values\n",
        "sdrs_before = []\n",
        "sdrs_after = []\n",
        "snrs_after = []\n",
        "snrs_before = []\n",
        "for noise_utterance, emb_utterance, clean_utterance, clean_utterance2  in test_csv:\n",
        "  noise_utterance = os.path.join('VoiceSplit',noise_utterance).replace(' ', '')\n",
        "  emb_utterance = os.path.join('VoiceSplit',emb_utterance).replace(' ', '')\n",
        "  clean_utterance = os.path.join('VoiceSplit',clean_utterance).replace(' ', '')\n",
        "  clean_utterance2 = os.path.join('VoiceSplit',clean_utterance2).replace(' ', '')\n",
        "  est_utterance = noise_utterance.replace('noisy', 'enhanced').replace(' ', '')\n",
        "\n",
        "  target_wav, _ = librosa.load(clean_utterance, sr=16000)\n",
        "  target_wav2, _ = librosa.load(clean_utterance2, sr=16000)\n",
        "  est_wav, _ = librosa.load(est_utterance, sr=16000)\n",
        "  mixed_wav, _ = librosa.load(noise_utterance, sr=16000)\n",
        "\n",
        "  len_est = len(est_wav)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest\n",
        "    est_wav = est_wav[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    est_wav = np.pad(est_wav, (0, len(mixed_wav)-len(est_wav)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  # get wav for second voice, its need for SDR calculation\n",
        "  est_wav2 = mixed_wav-est_wav\n",
        "\n",
        "  len_est = len(est_wav2)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest\n",
        "    est_wav2 = est_wav2[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    est_wav2 = np.pad(est_wav2, (0, len(mixed_wav)-len(est_wav2)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  len_est = len(target_wav)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest\n",
        "    target_wav = target_wav[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    target_wav = np.pad(target_wav, (0, len(mixed_wav)-len(target_wav)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  # get target_wav for second voice, its recomended because google dont provide clean_utterance2 in your demo i need get in LibreSpeech Dataset, but i dont know if they normalised this file..\n",
        "  target_wav2 = mixed_wav - target_wav\n",
        "  '''len_est = len(target_wav2)\n",
        "  len_mixed = len(mixed_wav)\n",
        "  if len_est > len_mixed:\n",
        "    # mixed need is biggest\n",
        "    target_wav2 = target_wav2[:len_mixed]\n",
        "  else:\n",
        "    # if mixed is biggest than estimation wav we need pad with zeros because is expected that this part is silence\n",
        "    target_wav2 = np.pad(target_wav2, (0, len(mixed_wav)-len(target_wav2)), 'constant', constant_values=(0, 0))'''\n",
        "\n",
        "\n",
        "  # calculate snr and sdr before model\n",
        "  ests = [torch.from_numpy(mixed_wav), torch.from_numpy(mixed_wav)] # the same voices is mixed_wav\n",
        "  egs = [torch.from_numpy(target_wav), torch.from_numpy(target_wav2)]\n",
        "  mix = torch.from_numpy(mixed_wav)\n",
        "  _snr, per = permute_SI_SNR(ests, egs, mix)\n",
        "  _sdr = permutation_sdr(ests, egs, mix, per)\n",
        "  snrs_before.append(_snr)\n",
        "  sdrs_before.append(_sdr)\n",
        "\n",
        "  # calculate snr and sdr after model\n",
        "  ests = [torch.from_numpy(est_wav), torch.from_numpy(est_wav2)]\n",
        "  egs = [torch.from_numpy(target_wav), torch.from_numpy(target_wav2)]\n",
        "  mix = torch.from_numpy(mixed_wav)\n",
        "  _snr, per = permute_SI_SNR(ests, egs, mix)\n",
        "  _sdr = permutation_sdr(ests, egs, mix, per)\n",
        "  snrs_after.append(_snr)\n",
        "  sdrs_after.append(_sdr)\n",
        "\n",
        "  # show in notebook results\n",
        "  print('-'*100)\n",
        "  print('-'*30,os.path.basename(noise_utterance),'-'*30)\n",
        "  print(\"Input/Noise Audio\")\n",
        "  display(Audio(mixed_wav,rate=16000))\n",
        "  print('Predicted Audio')\n",
        "  display(Audio(est_wav,rate=16000))\n",
        "  print('Target Audio')\n",
        "  display(Audio(target_wav,rate=16000))\n",
        "  print('Predicted2 Audio')\n",
        "  display(Audio(est_wav2,rate=16000))\n",
        "  print('Target2 Audio')\n",
        "  display(Audio(target_wav2,rate=16000))\n",
        "  print('-'*100)\n",
        "  del target_wav, est_wav, mixed_wav\n",
        "\n",
        "\n",
        "print('='*20,\"Before Model\",'='*20)\n",
        "print('\\nAverage SNRi: {:.5f}'.format(np.array(snrs_before).mean()))\n",
        "print('Average SDRi: {:.5f}'.format(np.array(sdrs_before).mean()))\n",
        "\n",
        "print('='*20,\"After Model\",'='*20)\n",
        "print('\\nAverage SNRi: {:.5f}'.format(np.array(snrs_after).mean()))\n",
        "print('Average SDRi: {:.5f}'.format(np.array(sdrs_after).mean()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}